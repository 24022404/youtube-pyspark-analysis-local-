"""
Preprocessing Module
Extract v√† t√°i s·ª≠ d·ª•ng logic t·ª´ file 01_preprocessing_final.ipynb
X·ª≠ l√Ω data: clean, transform, feature engineering
"""
from pyspark.sql import DataFrame
from pyspark.sql.functions import (
    col, to_timestamp, when, explode, length, size, split,
    hour, dayofweek, datediff, lit, regexp_replace, coalesce,
    unix_timestamp, from_unixtime
)
from functools import reduce
from config import YouTubeConfig
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class YouTubeDataPreprocessor:
    """
    Class x·ª≠ l√Ω preprocessing cho YouTube data
    T√°i s·ª≠ d·ª•ng logic t·ª´ notebook 01
    """
    
    def __init__(self, spark):
        self.spark = spark
        self.category_map = YouTubeConfig.CATEGORY_MAP
    
    def load_category_mapping(self, json_path):
        """
        Load category mapping t·ª´ JSON file
        """
        try:
            df_json = self.spark.read.json(json_path, multiLine=True)
            df_categories = df_json.select(explode(col("items")).alias("item")) \
                .select(
                    col("item.id").alias("categoryId"),
                    col("item.snippet.title").alias("category_name")
                )
            logger.info(f"‚úÖ Loaded {df_categories.count()} categories")
            return df_categories
        except Exception as e:
            logger.error(f"‚ùå Error loading categories: {e}")
            return None
    
    def clean_data(self, df: DataFrame) -> DataFrame:
        """
        B∆∞·ªõc 1: L√†m s·∫°ch d·ªØ li·ªáu
        - X√≥a rows c√≥ t·∫•t c·∫£ gi√° tr·ªã null
        - X√≥a rows c√≥ video_id null
        - Fill null cho description
        """
        logger.info("üßπ Cleaning data...")
        
        # X√≥a rows c√≥ t·∫•t c·∫£ gi√° tr·ªã null
        df = df.filter(
            reduce(lambda a, b: a | b, (col(c).isNotNull() for c in df.columns))
        )
        
        # X√≥a rows kh√¥ng c√≥ video_id
        df = df.filter(col("video_id").isNotNull() & (col("video_id") != ""))
        
        # Fill null cho description
        df = df.fillna({"description": "No description"})
        
        # Drop duplicates
        df = df.dropDuplicates(["video_id", "trending_date"])
        
        logger.info(f"‚úÖ Cleaned data: {df.count()} rows")
        return df
    
    def fix_timestamps(self, df: DataFrame) -> DataFrame:
        """
        B∆∞·ªõc 2: S·ª≠a timestamp format
        T·ª´ file 01: trending_date c√≥ format 'yyyy-MM-dd HH:mm:ss' thay v√¨ ISO format
        """
        logger.info("üïê Fixing timestamps...")
        
        # Fix trending_date: t·ª´ 'yyyy-MM-dd HH:mm:ss' -> timestamp
        df = df.withColumn(
            'trending_date',
            to_timestamp(col('trending_date'), 'yyyy-MM-dd HH:mm:ss')
        )
        
        # Fix publishedAt: t·ª´ 'yyyy-MM-dd HH:mm:ss' -> timestamp
        df = df.withColumn(
            'publishedAt',
            to_timestamp(col('publishedAt'), 'yyyy-MM-dd HH:mm:ss')
        )
        
        # L·ªçc b·ªè c√°c rows c√≥ timestamp kh√¥ng h·ª£p l·ªá
        df = df.filter(
            col('trending_date').isNotNull() & col('publishedAt').isNotNull()
        )
        
        logger.info("‚úÖ Timestamps fixed")
        return df
    
    def add_category_names(self, df: DataFrame, category_df: DataFrame = None) -> DataFrame:
        """
        B∆∞·ªõc 3: Th√™m category names
        """
        logger.info("üè∑Ô∏è Adding category names...")
        
        if category_df is None:
            # T·∫°o category mapping t·ª´ config
            from pyspark.sql.types import StructType, StructField, StringType
            schema = StructType([
                StructField("categoryId", StringType(), True),
                StructField("category_name", StringType(), True)
            ])
            category_data = [(k, v) for k, v in self.category_map.items()]
            category_df = self.spark.createDataFrame(category_data, schema)
        
        # Convert categoryId to string for joining
        df = df.withColumn("categoryId", col("categoryId").cast("string"))
        
        # Join v·ªõi categories
        df = df.join(
            category_df,
            df.categoryId == category_df.categoryId,
            "left"
        ).drop(category_df.categoryId)
        
        logger.info("‚úÖ Category names added")
        return df
    
    def feature_engineering(self, df: DataFrame) -> DataFrame:
        """
        B∆∞·ªõc 4: Feature Engineering
        T·∫°o c√°c features m·ªõi cho ML model
        """
        logger.info("‚öôÔ∏è Feature engineering...")
        
        # 1. Engagement rate
        df = df.withColumn(
            "engagement_rate",
            when(col("view_count") > 0,
                 (col("likes") + col("comment_count")) / col("view_count")
            ).otherwise(0)
        )
        
        # 2. Like ratio
        df = df.withColumn(
            "like_ratio",
            when(col("view_count") > 0,
                 col("likes") / col("view_count")
            ).otherwise(0)
        )
        
        # 3. Comment ratio
        df = df.withColumn(
            "comment_ratio",
            when(col("view_count") > 0,
                 col("comment_count") / col("view_count")
            ).otherwise(0)
        )
        
        # 4. Time features t·ª´ publishedAt
        df = df.withColumn("publish_hour", hour(col("publishedAt")))
        df = df.withColumn("publish_day_of_week", dayofweek(col("publishedAt")))
        
        # 5. Days to trending (t·ª´ publish ƒë·∫øn trending)
        df = df.withColumn(
            "days_to_trending",
            datediff(col("trending_date"), col("publishedAt"))
        )
        
        # 6. Title length
        df = df.withColumn("title_length", length(col("title")))
        
        # 7. Tags count
        df = df.withColumn(
            "tags_count",
            when(col("tags").isNotNull(),
                 size(split(col("tags"), "\\|"))
            ).otherwise(0)
        )
        
        # 8. Description length
        df = df.withColumn("description_length", length(col("description")))
        
        # 9. Has tags flag
        df = df.withColumn(
            "has_tags",
            when(col("tags").isNotNull() & (col("tags") != ""), 1).otherwise(0)
        )
        
        logger.info("‚úÖ Feature engineering completed")
        return df
    
    def preprocess_pipeline(self, df: DataFrame, category_json_path: str = None) -> DataFrame:
        """
        Pipeline ƒë·∫ßy ƒë·ªß: Clean -> Fix timestamps -> Add categories -> Feature engineering
        """
        logger.info("üöÄ Starting preprocessing pipeline...")
        
        # Load categories n·∫øu c√≥
        category_df = None
        if category_json_path:
            category_df = self.load_category_mapping(category_json_path)
        
        # Apply pipeline
        df = self.clean_data(df)
        df = self.fix_timestamps(df)
        df = self.add_category_names(df, category_df)
        df = self.feature_engineering(df)
        
        logger.info(f"‚úÖ Preprocessing completed: {df.count()} rows, {len(df.columns)} columns")
        return df
    
    def preprocess_realtime_record(self, record: dict) -> dict:
        """
        X·ª≠ l√Ω m·ªôt record real-time t·ª´ Kafka
        Lightweight preprocessing cho streaming data
        """
        try:
            # Parse timestamps
            from datetime import datetime
            
            if 'publishedAt' in record:
                record['publishedAt'] = datetime.fromisoformat(
                    record['publishedAt'].replace('Z', '+00:00')
                )
            
            # Add current timestamp as trending_date
            record['trending_date'] = datetime.now()
            
            # Calculate engagement rate
            view_count = record.get('view_count', 0)
            likes = record.get('likes', 0)
            comments = record.get('comment_count', 0)
            
            if view_count > 0:
                record['engagement_rate'] = (likes + comments) / view_count
                record['like_ratio'] = likes / view_count
                record['comment_ratio'] = comments / view_count
            else:
                record['engagement_rate'] = 0
                record['like_ratio'] = 0
                record['comment_ratio'] = 0
            
            # Add category name
            category_id = str(record.get('categoryId', ''))
            record['category_name'] = self.category_map.get(category_id, 'Unknown')
            
            # Title length
            record['title_length'] = len(record.get('title', ''))
            
            # Tags count
            tags = record.get('tags', '')
            record['tags_count'] = len(tags.split('|')) if tags else 0
            
            # Description length
            record['description_length'] = len(record.get('description', ''))
            
            return record
            
        except Exception as e:
            logger.error(f"Error preprocessing record: {e}")
            return record


def get_ml_features(df: DataFrame) -> DataFrame:
    """
    Ch·ªçn features c·∫ßn thi·∫øt cho ML model
    """
    feature_cols = [
        'video_id',
        'title',
        'channelTitle',
        'category_name',
        'view_count',
        'likes',
        'comment_count',
        'engagement_rate',
        'like_ratio',
        'comment_ratio',
        'publish_hour',
        'publish_day_of_week',
        'days_to_trending',
        'title_length',
        'tags_count',
        'description_length',
        'trending_date',
        'publishedAt'
    ]
    
    return df.select(*[col for col in feature_cols if col in df.columns])


# ========================================
# HELPER FUNCTIONS
# ========================================
def dataframe_info(df: DataFrame, name: str = "DataFrame"):
    """
    Hi·ªÉn th·ªã th√¥ng tin DataFrame (t∆∞∆°ng t·ª± notebook 01)
    """
    print("=" * 60)
    print(f"{name} INFO")
    print("=" * 60)
    print(f"Rows: {df.count()}, Columns: {len(df.columns)}")
    print("\nSchema:")
    df.printSchema()
    print("\nNull counts:")
    df.select([
        (col(c).isNull().cast("int")).alias(c) for c in df.columns
    ]).agg(*[
        sum(col(c)).alias(c) for c in df.columns
    ]).show()
    print("=" * 60)


if __name__ == '__main__':
    # Test preprocessing
    from pyspark.sql import SparkSession
    
    spark = SparkSession.builder \
        .appName("PreprocessingTest") \
        .master("local[*]") \
        .getOrCreate()
    
    preprocessor = YouTubeDataPreprocessor(spark)
    print("‚úÖ Preprocessor initialized successfully")
    
    spark.stop()