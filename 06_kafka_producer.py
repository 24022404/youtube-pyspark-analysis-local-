"""
06_kafka_producer.py
Kafka Producer - Stream YouTube trending data v√†o Kafka topic
"""

from kafka import KafkaProducer
import json
import time
import pandas as pd
from datetime import datetime
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class YouTubeKafkaProducer:
    def __init__(self, 
                 bootstrap_servers='localhost:9092',
                 topic='youtube-trending',
                 csv_file='./data/raw_data.csv'):
        """
        Kh·ªüi t·∫°o Kafka Producer
        
        Args:
            bootstrap_servers: Kafka server address
            topic: Kafka topic name
            csv_file: ƒê∆∞·ªùng d·∫´n file CSV ch·ª©a data
        """
        self.topic = topic
        self.csv_file = csv_file
        
        # Kh·ªüi t·∫°o Kafka Producer
        try:
            self.producer = KafkaProducer(
                bootstrap_servers=bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                acks='all',  # ƒê·∫£m b·∫£o message ƒë∆∞·ª£c ghi th√†nh c√¥ng
                retries=3,
                max_in_flight_requests_per_connection=1
            )
            logger.info(f"‚úÖ Kafka Producer ƒë√£ k·∫øt n·ªëi ƒë·∫øn {bootstrap_servers}")
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi Kafka: {e}")
            raise
    
    def load_data(self):
        """Load data t·ª´ CSV"""
        try:
            df = pd.read_csv(self.csv_file)
            logger.info(f"üìä ƒê√£ load {len(df):,} videos t·ª´ {self.csv_file}")
            return df
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë·ªçc CSV: {e}")
            raise
    
    def prepare_message(self, row):
        """
        Chu·∫©n b·ªã message ƒë·ªÉ g·ª≠i v√†o Kafka
        
        Args:
            row: Pandas Series (1 d√≤ng t·ª´ DataFrame)
        Returns:
            dict: Message d·∫°ng JSON
        """
        return {
            'video_id': str(row['video_id']),
            'title': str(row['title']),
            'publishedAt': str(row['publishedAt']),
            'channelId': str(row['channelId']),
            'channelTitle': str(row['channelTitle']),
            'categoryId': int(row['categoryId']),
            'trending_date': str(row['trending_date']),
            'tags': str(row['tags']) if pd.notna(row['tags']) else '',
            'view_count': int(row['view_count']),
            'likes': int(row['likes']),
            'dislikes': int(row['dislikes']),
            'comment_count': int(row['comment_count']),
            'thumbnail_link': str(row['thumbnail_link']),
            'comments_disabled': bool(row['comments_disabled']),
            'ratings_disabled': bool(row['ratings_disabled']),
            'description': str(row['description']) if pd.notna(row['description']) else '',
            'kafka_timestamp': datetime.now().isoformat()
        }
    
    def stream_data(self, batch_size=10, delay=2):
        """
        Stream data theo batch v√†o Kafka
        
        Args:
            batch_size: S·ªë message m·ªói batch
            delay: Th·ªùi gian delay gi·ªØa c√°c batch (gi√¢y)
        """
        df = self.load_data()
        total_records = len(df)
        sent_count = 0
        
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu streaming {total_records:,} videos...")
        logger.info(f"   Batch size: {batch_size}")
        logger.info(f"   Delay: {delay}s")
        
        try:
            for idx, row in df.iterrows():
                # Chu·∫©n b·ªã message
                message = self.prepare_message(row)
                
                # G·ª≠i v√†o Kafka
                future = self.producer.send(self.topic, value=message)
                
                # ƒê·ª£i x√°c nh·∫≠n (t√πy ch·ªçn)
                try:
                    record_metadata = future.get(timeout=10)
                    sent_count += 1
                    
                    # Log m·ªói batch
                    if sent_count % batch_size == 0:
                        logger.info(
                            f"üì§ ƒê√£ g·ª≠i {sent_count}/{total_records} videos "
                            f"({sent_count/total_records*100:.1f}%) | "
                            f"Topic: {record_metadata.topic} | "
                            f"Partition: {record_metadata.partition}"
                        )
                        time.sleep(delay)
                
                except Exception as e:
                    logger.error(f"‚ùå L·ªói khi g·ª≠i video {message['video_id']}: {e}")
            
            # ƒê·∫£m b·∫£o t·∫•t c·∫£ message ƒë∆∞·ª£c g·ª≠i
            self.producer.flush()
            logger.info(f"‚úÖ HO√ÄN TH√ÄNH! ƒê√£ g·ª≠i {sent_count}/{total_records} videos")
            
        except KeyboardInterrupt:
            logger.warning(f"‚ö†Ô∏è D·ª´ng streaming! ƒê√£ g·ª≠i {sent_count}/{total_records} videos")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi streaming: {e}")
        finally:
            self.close()
    
    def stream_realtime_simulation(self, interval=5):
        """
        Gi·∫£ l·∫≠p streaming real-time (g·ª≠i data m·ªõi nh·∫•t li√™n t·ª•c)
        
        Args:
            interval: Th·ªùi gian delay gi·ªØa c√°c message (gi√¢y)
        """
        df = self.load_data()
        
        # L·∫•y 50 videos m·ªõi nh·∫•t
        df_recent = df.tail(50)
        
        logger.info(f"üîÑ B·∫Øt ƒë·∫ßu real-time simulation v·ªõi {len(df_recent)} videos")
        logger.info(f"   Interval: {interval}s")
        
        try:
            count = 0
            while True:
                # L·∫∑p qua c√°c video
                for idx, row in df_recent.iterrows():
                    message = self.prepare_message(row)
                    
                    # Update timestamp m·ªõi
                    message['kafka_timestamp'] = datetime.now().isoformat()
                    
                    # G·ª≠i v√†o Kafka
                    self.producer.send(self.topic, value=message)
                    count += 1
                    
                    logger.info(
                        f"üì° [{count}] Sent: {message['title'][:50]}... | "
                        f"Views: {message['view_count']:,}"
                    )
                    
                    time.sleep(interval)
        
        except KeyboardInterrupt:
            logger.warning(f"‚ö†Ô∏è D·ª´ng simulation! ƒê√£ g·ª≠i {count} messages")
        finally:
            self.close()
    
    def close(self):
        """ƒê√≥ng Kafka Producer"""
        if self.producer:
            self.producer.close()
            logger.info("üëã ƒê√£ ƒë√≥ng Kafka Producer")


def main():
    """Main function"""
    print("=" * 60)
    print("KAFKA PRODUCER - YOUTUBE TRENDING DATA")
    print("=" * 60)
    
    # Kh·ªüi t·∫°o producer
    producer = YouTubeKafkaProducer(
        bootstrap_servers='localhost:9092',
        topic='youtube-trending',
        csv_file='./data/raw_data.csv'
    )
    
    # Menu l·ª±a ch·ªçn
    print("\nüìã Ch·ªçn ch·∫ø ƒë·ªô streaming:")
    print("   1. Batch streaming (g·ª≠i t·∫•t c·∫£ data)")
    print("   2. Real-time simulation (l·∫∑p li√™n t·ª•c)")
    
    choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1 ho·∫∑c 2): ").strip()
    
    if choice == '1':
        batch_size = int(input("Batch size (m·∫∑c ƒë·ªãnh 10): ") or 10)
        delay = float(input("Delay gi·ªØa c√°c batch (gi√¢y, m·∫∑c ƒë·ªãnh 2): ") or 2)
        producer.stream_data(batch_size=batch_size, delay=delay)
    
    elif choice == '2':
        interval = float(input("Interval gi·ªØa messages (gi√¢y, m·∫∑c ƒë·ªãnh 5): ") or 5)
        producer.stream_realtime_simulation(interval=interval)
    
    else:
        print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")


if __name__ == "__main__":
    main()
